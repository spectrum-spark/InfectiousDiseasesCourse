{
  "hash": "5ca600353ce123eb21f543d849ce682e",
  "result": {
    "markdown": "---\ntitle: \"COVID-19 Modelling in R\"\nauthor:\n    - Thanks to <a href=\"http://www.tropmedres.ac/researchers/researcher/lisa-white\">Lisa White</a>, Nuffield Department of Medicine, Oxford University,\n    - <a href=\"https://research.jcu.edu.au/portfolio/michael.meehan1/\">Michael Meehan</a>, Australian Institute of Tropical Health & Medicine, James Cook University and\n    - Adapted and presented by <a href=\"https://research.jcu.edu.au/portfolio/emma.mcbryde\">Emma McBryde</a>, Australian Institute of Tropical Health & Medicine, James Cook University\n    - July 2022\noutput:\n    html_document:\n        self_contained: yes\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n<style type=\"text/css\">\ntextarea {\n    width: 100%;\n}\n\n.line-block {\n    line-height: 1.5em;\n}\n\n.line-block a {\n    font-style: italic;\n}\n</style>\n:::\n\n# Summary\n\nA combination of theory and practice to extend the concepts of maximum likelihood and other model fitting methodologies to the context of transmission dynamic models.\n\nThis session is divided into four parts:\n\n**Part 1**: Thinking about parameter fitting - how to measure fit and how to find the best fit\n\n**Part 2**: Apply measures of distance to our data: least squares and maximum likelihood\n\n**Part 3**: Parameter estimation techniques with maximum likelihood\n\n**Part 4**: Confidence intervals derived from likelihood methods\n\n# Model fitting\nThe first exercise in this session is to fit a model \"by eye\". We hope that by the end of the exercise you will be thinking to yourself \"there must be an easier and more robust way to do this\". This of course motivates the rest of the session. First open Shiny application Least_Squares_App.Rmd. (App can be accessed online at https://moru.shinyapps.io/Least_Squares_App/). In this application the dots represent the data and the line represents predicted values based on a mathematical model (in this case a simple cubic equation). This application allows you to explore how changing model parameters changes SSQ (sum of squares of residuals). It is intended to give you an intuitive feel for how a model fitting approach that aims to minimize the SSQs works.\n\nTry to find the best fitting line by adjusting the three parameters with the sliders. Check your answer with the application.\n\n## Questions\nWhat are the residuals?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nWhat disadvantages might there be of fitting models by hand like this?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nCan you think of a simple algorithm for searching over all the parameters and finding those that give the best fitting model?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nIn the example we have been minimizing the sum of squares of the residuals. What would be the effect if we decided to minimize the sums of the absolute values of the residuals? Would you expect to get the same answer?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\n# Distance Functions\nIn part 2 we consider different ways of measuring how far our model results are from the data. We use the model developed in the last session and the initial data in March 2020. We are going to briefly touch on sum of squares and then use likelihood functions to assess model fit to data.\n\nIn this practical we are going to estimate some of the parameters of our base SEIR COVID-19 model by fitting it to our incidence data.\nWhat are the free parameters of our base model?\n\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nIn this exercise let us assume a known latent period and infectious period and use the basic reproduction number, $R_0$, and the initial size of the infected population, $I(t=0)$, as the free parameters we will calibrate using our data.\n\nTo begin, create a new R script, save the file as `fitting_models.R` into your working directory, and copy the code chunk below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SPARK Modelling Short course\n\n#########################\n## FITTING MODELS IN R ##\n#########################\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(deSolve)\nlibrary(ggplot2)\n```\n:::\n\n\n## Filtering the data\nWhen calibrating each of these parameters, one thing we must be careful of is to account for the introduction of various interventions as these will likely have a significant impact on the parameter values (the reproduction number in particular). Therefore, in the code provided below I first isolate the \"pre-intervention\" era (which we defined in the last session as the first wave betweeen 7th and 26th March 2020. We will use this truncated dataset to estimate the parameters, $R_0$ and `initial_infected_population`. The code below should let you view the data we are using to fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data imports and filtering\nfirst_wave <- read.csv(\"first_wave_TH.csv\", colClasses = c(\"Date\", \"numeric\", \"numeric\"))\n# fixing the format of date\nfirst_wave$Date <- as.Date(first_wave$Date, format = \"%Y-%m-%d\")\n# Time window\nstart_date <- as.Date(\"2020-03-07\")\nend_date <- as.Date(\"2020-03-26\")\n\n# Filter data to capture period prior to interventions. This has already been done in the last session.\nuncontrolled_period <- first_wave %>%\n    filter(Date >= start_date, Date <= end_date)\n\n# Plot the filtered data\nggplot(uncontrolled_period) +\n    geom_col(aes(x = Date, y = Cases), width = 1, fill = \"dodgerblue2\", colour = \"blue\") +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Uncontrolled first wave of COVID-19 in Thailand 1st to 24th March, 2020\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/load-data-1.png){width=672}\n:::\n:::\n\n\nWe'll re-use the functions from the last session\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Define model as per the previous session\n# Time window\ntimes <- seq(start_date, end_date, by = 1)\n\n# Model parameters\nparameters <- c(\n    R0 = 4,\n    incubation_period = 5,\n    infectious_period = 6\n)\n\n# Initial conditions\nTotal_population <- 6.6e7 # Population of Thailand\nInitial_exposed <- 0\nInitial_infectious <- 20 # Initial infectious seed\nInitial_recovered <- 0\nInitial_susceptible <- Total_population - Initial_exposed - Initial_infectious - Initial_recovered\n\n# State variables\nstate <- c(\n    Susceptible = Initial_susceptible,\n    Exposed = Initial_exposed,\n    Infectious = Initial_infectious,\n    Recovered = Initial_recovered\n)\n\n\n# Model function\nCOVID.base <- function(t, state, parameters) {\n    with(as.list(c(state, parameters)), {\n        # Calculate the total population size\n        Total_population <- Susceptible + Exposed + Infectious + Recovered\n\n        # Calculate the average force of infection imposed on each susceptible individual\n        force_of_infection <- R0 * Infectious / (Total_population * infectious_period)\n\n        # Calculate the net (instantaneous) change in each state variable\n        Susceptible_change <- -force_of_infection * Susceptible\n        Exposed_change <- force_of_infection * Susceptible - Exposed / incubation_period\n        Infectious_change <- Exposed / incubation_period - Infectious / infectious_period\n        Recovered_change <- Infectious / infectious_period\n\n        # Return net changes as list\n        return(list(\n            c(\n                Susceptible_change,\n                Exposed_change,\n                Infectious_change,\n                Recovered_change\n            )\n        ))\n    })\n}\n\nsolve.base.model <- function(y_ = state,\n                             times_ = times,\n                             func. = COVID.base,\n                             parms_ = parameters) {\n    out <- ode(\n        y = y_,\n        times = as.numeric(times_ - times_[1]),\n        func = func.,\n        parms = parms_\n    )\n\n    # Calculate the prevalence, incidence and cumulative incidence (for comparison with data)\n    out <- as.data.frame(out) %>%\n        mutate(\n            Prevalence = Exposed + Infectious,\n            Incidence = Exposed / parms_[\"incubation_period\"],\n            Cumulative_incidence = cumsum(Incidence) + Incidence[1],\n            Population = Susceptible + Exposed + Infectious + Recovered,\n            Date = times_\n        )\n\n    return(out)\n}\n```\n:::\n\n    \n## Solving the model for a specific set of parameters\nNow we use the model developed in the last session and solve the model for an initial set of parameters, an initial state and using dates from 7th - 26th March, 2020\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# now we use the model developed in the last session and solve it\n# for an initial set of parameters, an initial state and using times\n# from just 7th - 26th  of March 2020\nout_init <- solve.base.model(\n    y_ = state,\n    times_ = times,\n    func. = COVID.base,\n    parms_ = parameters\n)\n\n# Plot the filtered data and the first model \"guess\"\nggplot(uncontrolled_period) +\n    geom_col(aes(x = Date, y = Cases), width = 1, fill = \"dodgerblue2\", colour = \"blue\") +\n    geom_point(data = out_init, aes(x = Date, y = Incidence), size = 2, colour = \"firebrick2\") +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Thailand's First Wave, Jan-Jul 2020\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/initial-guess-1.png){width=672}\n:::\n:::\n\n  \n##   Calculating \"distance\" between the model and data\n We  can use the model output, stored in out_init to assess model fit. We can use sum of squares of the difference between the data and the model.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# find the sum of the residuals squared for day 2 until day 24\nSSQ_initial <- sum((uncontrolled_period$Cases[-1] - out_init$Incidence[-1])^2)\nSSQ_initial\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24516.09\n```\n:::\n:::\n\n\nAlternatively, we can use the output from our last function call to calculate the *likelihood* of our observed data given our COVID model (which of course is specific to the current parameter values that we have used). To do this, a common practice is to assume that the observed daily incidence (stored in the dataframe `uncontrolled_period`) is a Poisson-distributed random variable with mean given by our model-predicted incidence (stored in the dataframe `out_init`). Under this model the (log-)likelihood of each daily observation can be calculated using the dpois() function; and the total (log-)likelihood for all of the observations is simply the sum of all these:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the negative log-likelihood for our initial parameter values, assuming a Poisson residual distribution\npoisson_negative_log_likelihood_initial <- -sum(dpois(uncontrolled_period$Cases[-1],\n    out_init$Incidence[-1],\n    log = TRUE\n))\npoisson_negative_log_likelihood_initial\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 212.8381\n```\n:::\n:::\n\n  \n Th ere are many possible distributions that can be used to develop some measure of distance between the data and the model expectation. You may wish to investigate some of the density distributions available in R, which you can do by typing `?distributions` in your console.\n\nAnother distribution, which allows us to account for high variability in the data is the negative binomial distribution. We could use the output from our last function call to calculate the likelihood of our observed data given our COVID model using `dnbinom()` function in much the same way as the Poisson distribution. Those with a particular interest in statistics and probability distributions may wish to experiment and to consider the pros and cons of different choices. For the remainder of this exercise, however, we will continue with the Poisson distribution which has the attribute of having only one parameter to estimate, the mean.\n\n## Transforming the parameters\nBefore launching into a likelihood-based search across all parameter space for the values of $R_0$ and $I(t=0)$ that minimize our negative log-likelihood (which is equivalent to maximizing the positive log-likelihood), it would behoove us to pause and plan our search carefully. We will use the optim() function to minimize the log-likelihood, but we should be cogniscent of the fact that this function searches over all conceivable values of the parameters (i.e., from $-\\infty$ to $\\infty$)).\n\nWhat are the feasible ranges of values for our two parameters: $R_0$ and $I(t=0)$?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\n\nWith this in mind, it would be sensible to restrict our search to only those values of parameters that are feasible. Not only should this increase the speed of the algorithm (hopefully leading to the desired answer more quickly) but may also help us avoid any computational errors that might arise which will kill our search prematurely (for instance, `dpois()` and `dnbinom()` may have a difficult time calculating the probability density for negative observations!). For this purpose, we introduce a set of transformed parameters, that is parameters that exist in the full interval $-\\infty$ to $\\infty$, but can be mapped back to our feasible regions for our parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# transform the parameters\n# The search / optimization algorithm we employ searches over\n# the range (-Inf, Inf) for each variable. In our application\n# we are only interested in solutions in the range (0, Inf)\n# for R0 and I(0). Therefore, it\n# would be a good idea to apply a transformation to our\n# search space so that we are not wasting time exploring\n# infeasible regions of parameter space.\ninitial_transformed_parameters <- log(c(\n    \"R0\" = 4,\n    \"Initial_infectious\" = 20\n))\n```\n:::\n\n\nSo now we are equipped to fit our first maximum likelihood optimisation, using code like below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Fitting routine ####\n\n# We have selected R0 and I(0) as our free parameters\n# We need a function that accepts R0 and I(0) as arguments,\n# solves the model using these inputs, and calculates the\n# negative log-likelihood of the data given these parameters\n\nnegative_log_likelihood <- function(transformed_parameters,\n                                    data = uncontrolled_period$Cases[-1],\n                                    state_base = state,\n                                    times_ = times,\n                                    func. = COVID.base,\n                                    parms_base = parameters) {\n\n    # Untransform parameters\n    R0 <- exp(transformed_parameters[\"R0\"])\n    Initial_infectious <- exp(transformed_parameters[\"Initial_infectious\"])\n\n    # Calculate updated susceptible population\n    Initial_susceptible <- state_base[\"Susceptible\"] + state_base[\"Infectious\"] - Initial_infectious\n\n    # Overwrite baseline parameters with proposed parameters\n    parms_base[\"R0\"] <- R0\n    state_base[\"Susceptible\"] <- Initial_susceptible\n    state_base[\"Infectious\"] <- Initial_infectious\n\n    # Solve model with updated parameters\n    out <- solve.base.model(\n        state_base,\n        times_,\n        func.,\n        parms_base\n    )\n\n    return(-sum(dpois(\n        x = data,\n        lambda = out$Incidence[-1],\n        log = TRUE\n    )))\n}\n```\n:::\n\n    \nWe can check it works as intended:\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson_negative_log_likelihood_initial_from_function <- negative_log_likelihood(initial_transformed_parameters)\npoisson_negative_log_likelihood_initial_from_function\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 212.8381\n```\n:::\n\n```{.r .cell-code}\npoisson_negative_log_likelihood_initial\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 212.8381\n```\n:::\n:::\n\n\n# Parameter estimation with maximum likelihood\nWith our likelihood function now set up, we can use the `optim` function to determine the optimal parameters, that is the parameter values that minimize the negative log-likelihood.\n\nWe will use the Nelder-Mead search algorithm. This is a search algorithm that allows rapid exploration of the parameter space for the optimal parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Optimal parameters ####\n\n# Use the optim function to determine the parameters that\n# minimize the negative log-likelihood (i.e., maximize\n# the likelihood)\n# We will use the Nelder-Mead optimization solver\noptim_NM <- optim(\n    par = initial_transformed_parameters,\n    fn = negative_log_likelihood,\n    control = list(maxit = 500),\n    method = \"Nelder-Mead\",\n    hessian = TRUE\n)\n```\n:::\n\nOnce the search is complete, inspect the solution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for convergence (always code 0 for NM)\noptim_NM$convergence # 0 - converged; 1 - failed to converge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# Inspect solution\noptim_NM$par\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                R0 Initial_infectious \n          1.378931           3.413004 \n```\n:::\n:::\n\nWhat are the initial values of the two fitted parameters?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nThe results are a little surprising. However, remember that our search was conducted with reference to the transformed parameters; in order to retrieve the true optimal parameter values we must invert the transformation:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Back-transform parameters\noptimum_parameters <- exp(optim_NM$par)\noptimum_parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                R0 Initial_infectious \n          3.970654          30.356305 \n```\n:::\n:::\n\n\nBut have we improved our likelihood from the initial guess we had?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect the optimal negative log-likelihood\noptimum_NLL <- optim_NM$value\noptimum_NLL\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 144.8615\n```\n:::\n:::\n\nHow does the likelihood score at the optimal value compare with our initial likelihood score (`poisson_negative_log_likelihood_initial`)?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nSatisfied that we have found the optimal solution, let's now solve the model using these parameter values and compare the solution with our observed data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Optimal model solution ####\n\n# Create the optimal parameter and state vectors\noptimal_parameters <- parameters\noptimal_parameters[\"R0\"] <- optimum_parameters[\"R0\"]\n\noptimal_initial_state <- state\noptimal_initial_state[\"Susceptible\"] <- state[\"Susceptible\"] + state[\"Infectious\"] - optimum_parameters[\"Initial_infectious\"]\noptimal_initial_state[\"Infectious\"] <- optimum_parameters[\"Initial_infectious\"]\n\n# Solve the model given the optimal parameters and initial conditions\noptimal_solution <- solve.base.model(\n    y_ = optimal_initial_state,\n    times_ = times,\n    func. = COVID.base,\n    parms = optimal_parameters\n)\n\n\n# Plot the optimal solution\nggplot(first_wave) +\n    geom_col(aes(x = Date, y = Cases), width = 1, fill = \"dodgerblue2\", colour = \"blue\") +\n    geom_point(data = optimal_solution, aes(x = Date, y = Incidence), size = 2, colour = \"firebrick2\") +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Fit to unmitigated period\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/optimal-fit-with-data-1.png){width=672}\n:::\n:::\n\n\nBonus code: copy and paste this code to perform a grid search -getting the negative log likelihood across a sequence of each parameter value.\n\n::: {.cell hash='session5_fitting_models_in_r_cache/html/grid-search_356a454811d102e93ff7edddef9fcbec'}\n\n```{.r .cell-code}\n# Specify search grid\nR0_vec <- seq(from = 0.1, to = 10, by = 0.1)\ninitial_infectious_vec <- seq(from = 1, to = 50, by = 1)\n\n# the function that will perform grid search\n\nnll.grid <- function(R0_vec, initial_infectious_vec) {\n    parameter_grid <- expand.grid(\n        R0 = R0_vec,\n        initial_infectious = initial_infectious_vec\n    )\n    nll <- data.frame(matrix(nrow = nrow(parameter_grid), ncol = 1))\n    colnames(nll) <- \"NLL\"\n    for (row_of_params in seq(1, dim(parameter_grid)[1])) {\n        transformed_params <- log(c(\n            \"R0\" = parameter_grid[row_of_params, 1],\n            \"Initial_infectious\" = parameter_grid[row_of_params, 2]\n        ))\n        nll[row_of_params, ] <- negative_log_likelihood(transformed_params)\n    }\n    return(data.frame(\n        Reproduction_number = parameter_grid[\"R0\"],\n        Initial_infectious = parameter_grid[\"initial_infectious\"],\n        nll\n    ))\n}\n\n# call the function that will calculate likelihood surface\nnll_grid_results <- nll.grid(R0_vec, initial_infectious_vec)\n\n\n# Plot likelihood surface\nnll_plot <- nll_grid_results %>%\n    ggplot2::ggplot(aes(\n        x = R0,\n        y = initial_infectious,\n        fill = NLL,\n        z = NLL\n    )) +\n    geom_tile() +\n    geom_contour(breaks = c(100, 110, 120, 130, 200, 500, 1000, 4000)) +\n    ylab(\"I(0)\") +\n    xlab(\"R0\") +\n    ggtitle(\"Likelihood surface\") +\n    theme_bw()\n\n# Extract optimum from grid search\ngrid_optimum <- nll_grid_results[which.min(nll_grid_results$NLL), ]\ngrid_optimum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     R0 initial_infectious      NLL\n2940  4                 30 144.9431\n```\n:::\n\n```{.r .cell-code}\n# Superimpose optimum on likelihood surface plot\nnll_plot <- nll_plot +\n    geom_point(aes(\n        x = grid_optimum$R0,\n        y = grid_optimum$initial_infectious\n    ),\n    colour = \"red\",\n    )\n# Superimpose optimal point to negative-log-likelihood plot\nnll_plot <- nll_plot + geom_point(aes(\n    x = optimum_parameters[\"R0\"],\n    y = optimum_parameters[\"Initial_infectious\"]\n),\ncolour = \"yellow\",\nsize = 1\n)\n\nprint(nll_plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/grid-search-1.png){width=672}\n:::\n:::\n\n\nWhy might a grid search not be the best option for finding optimum parameter values ?\n<textarea name='Text1' cols='127' rows='4'></textarea>\n\n# Calculating confidence intervals\nFirstly we are going to put some confidence intervals around the data. This is commonly done but has some problems that relate to model assumptions. For example, if we use the Poisson distribution for estimating parameters, it assumes the spread of the data has a variance equal to the mean. The Negative Binomial distribution allows for greater data variance, but we have one extra parameter to estimate -the variance of the negative binomial distribution itself. Below is some code to examine the confidence intervals around your data with the caveats given above.\n\nFirstly, calculate the Poisson distribution centred on the optimised model solution. Find the inter-quartile range and 95% confidence interval of this distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the observational confidence intervals\noptimal_solution <- optimal_solution %>%\n    mutate(\n        lower50 = qpois(p = 0.25, lambda = Incidence), # 50% confidence interval (i.e., 25 - 75th centiles)\n        upper50 = qpois(p = 0.75, lambda = Incidence),\n        lower95 = qpois(p = 0.025, lambda = Incidence), # 95% confidence interval (i.e., 2.5 - 97.5th centiles)\n        upper95 = qpois(p = 0.975, lambda = Incidence)\n    )\n\n# Plot confidence intervals as ribbons around the central estimates\nggplot(first_wave) +\n    geom_col(aes(x = Date, y = Cases), width = 1.0, fill = \"dodgerblue2\", colour = \"blue\") +\n    geom_ribbon(data = optimal_solution[-1, ], aes(x = Date, ymin = lower50, ymax = upper50), fill = \"firebrick2\", colour = \"firebrick2\", alpha = 0.8) +\n    geom_ribbon(data = optimal_solution[-1, ], aes(x = Date, ymin = lower95, ymax = upper95), fill = \"firebrick2\", colour = \"firebrick2\", alpha = 0.5) +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Fit to unmitigated period (with observational uncertainty)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/observation-uncertainty-1.png){width=672}\n:::\n:::\n\n\nMore important than confidence intervals around the data are confidence intervals around the estimated parameters. We found an optimum value for the estimated parameters using the R inbuilt optim function, but this function can do much more.\n\nIn addition to the point estimates for each of the parameters, we can also use the results of the optimization search to calculate confidence intervals for them too. The idea is to use the curvature of the likelihood surface to determine the range of parameter values that exceed a particular likelihood score (pointer likelihood surfaces should lead to more smaller confidence intervals). The curvature values are stored in what is known as the Hessian matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim_NM$hessian\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          R0 Initial_infectious\nR0                 10561.786          3159.1981\nInitial_infectious  3159.198           994.9545\n```\n:::\n:::\n\n\nThe covariance matrix is the inverse of this matrix, and be calculated using:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Covariance matrix\ncovar_matrix <- solve(optim_NM$hessian)\ncovar_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             R0 Initial_infectious\nR0                  0.001884512       -0.005983737\nInitial_infectious -0.005983737        0.020004746\n```\n:::\n:::\n\n\nUsing the covariance matrix, we can next calculate the standard errors for each parameter by taking the square root of the diagonal elements of the covariance matrix, and then adding and subtracting these to the central estimates to obtain the 95% confidence intervals:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now use the covariance matrix to generate standard errors for each parameter\nstd_errors <- sqrt(diag(covar_matrix))\nestimates_transformed <- data.frame(\n    estimate = optim_NM$par,\n    std_error = std_errors\n) %>%\n    mutate(\n        lower95CI = estimate - 1.96 * std_error,\n        upper95CI = estimate + 1.96 * std_error\n    )\nestimates_transformed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   estimate  std_error lower95CI upper95CI\nR0                 1.378931 0.04341097  1.293845  1.464016\nInitial_infectious 3.413004 0.14143813  3.135785  3.690223\n```\n:::\n\n```{.r .cell-code}\n# Back-transform estimates and confidence intervals\nestimates <- exp(estimates_transformed[, -2])\nestimates\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    estimate lower95CI upper95CI\nR0                  3.970654  3.646783  4.323288\nInitial_infectious 30.356305 23.006700 40.053777\n```\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}