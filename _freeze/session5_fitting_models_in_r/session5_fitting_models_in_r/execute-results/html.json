{
  "hash": "dad5e1f60ddce189a1aecf70fc6154bc",
  "result": {
    "markdown": "---\ntitle: \"Modelling an outbreak - COVID-19 in Thailand\"\nauthor:\n    - Thanks to <a href=\"http://www.tropmedres.ac/researchers/researcher/lisa-white\">Lisa White</a>, Nuffield Department of Medicine, Oxford University,\n    - <a href=\"https://research.jcu.edu.au/portfolio/michael.meehan1/\">Michael Meehan</a>, and <a href=\"https://research.jcu.edu.au/portfolio/emma.mcbryde\">Emma McBryde</a>, Australian Institute of Tropical Health & Medicine, James Cook University.\n    - Adapted and presented by [Eamon Conway](https://github.com/EamonConway), The Walter and Eliza Hall Institute, Melbourne.\ndate: March 2023\ndate-format: MMMM YYYY\noutput:\n    html_document:\n        self_contained: yes\nautocomplete: false\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n<style type=\"text/css\">\ntextarea {\n    width: 100%;\n}\n\n.line-block {\n    line-height: 1.5em;\n}\n\n.line-block a {\n    font-style: italic;\n}\n</style>\n:::\n\n# Summary\n\nA combination of theory and practice to extend the concepts of maximum likelihood and other model fitting methodologies to the context of transmission dynamic models.\n\nThis session is divided into four parts:\n\n**Part 1**: Thinking about parameter fitting - how to measure fit and how to find the best fit\n\n**Part 2**: Apply measures of distance to our data: least squares and maximum likelihood\n\n**Part 3**: Parameter estimation techniques with maximum likelihood\n\n**Part 4**: Confidence intervals derived from likelihood methods\n\n# Model fitting\nThe first exercise in this session is to fit a model \"by eye\". We hope that by the end of the exercise you will be thinking to yourself \"there must be an easier and more robust way to do this\". This of course motivates the rest of the session. First open Shiny application Least_Squares_App.Rmd. (App can be accessed online [here](https://moru.shinyapps.io/Least_Squares_App/)). In this application the dots represent the data and the line represents predicted values based on a mathematical model (in this case a simple cubic equation). This application allows you to explore how changing model parameters changes SSQ (sum of squares of residuals). It is intended to give you an intuitive feel for how a model fitting approach that aims to minimize the SSQs works.\n\nTry to find the best fitting line by adjusting the three parameters with the sliders. Check your answer with the application.\n\n## Questions\nWhat are the residuals?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nWhat disadvantages might there be of fitting models by hand like this?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nCan you think of a simple algorithm for searching over all the parameters and finding those that give the best fitting model?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nIn the example we have been minimizing the sum of squares of the residuals. What would be the effect if we decided to minimize the sums of the absolute values of the residuals? Would you expect to get the same answer?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\n# Distance Function\nIn this section we will use the sum of squares distance to see how far our model results are from the data. To do so, we will use the model developed in the last session and the initial data in March 2020. To determine our parameters of interest we will minimise our sum of squares and assess model fit to data.\n\nIn this practical we are going to estimate some of the parameters of our base SEIR COVID-19 model by fitting it to our incidence data.\nWhat are the free parameters of our base model?\n\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nIn this exercise let us assume a known latent period and infectious period and use the basic reproduction number, $R_0$, and the initial size of the infected population, $I(t=0)$, as the free parameters we will calibrate using our data.\n\nTo begin, create a new R script, save the file as `fitting_models.R` into your working directory, and copy the code chunk below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SPARK Modelling Short course\n\n#########################\n## FITTING MODELS IN R ##\n#########################\nlibrary(dplyr)\nlibrary(deSolve)\nlibrary(ggplot2)\n```\n:::\n\n\n## Filtering the data\nWhen calibrating each of these parameters, one thing we must be careful of is to account for the introduction of various interventions as these will likely have a significant impact on the parameter values (the reproduction number in particular). Therefore, in the code provided below I first isolate the \"pre-intervention\" era (which we defined in the last session as the first wave between 7th and 26th March 2020. We will use this truncated dataset to estimate the parameters, $R_0$ and `initial_infected_population`. The code below should let you view the data we are using to fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data imports and filtering\nfirst_wave <- read.csv(\"first_wave_TH.csv\", colClasses = c(\"Date\", \"numeric\", \"numeric\"))\n# fixing the format of date\nfirst_wave$Date <- as.Date(first_wave$Date, format = \"%Y-%m-%d\")\n# Time window\nstart_date <- as.Date(\"2020-03-07\")\nend_date <- as.Date(\"2020-03-26\")\n\n# Filter data to capture period prior to interventions. This has already been done in the last session.\nuncontrolled_period <- first_wave %>%\n    filter(Date >= start_date, Date <= end_date)\n\n# Plot the filtered data\nggplot(uncontrolled_period) +\n    geom_col(aes(x = Date, y = Cases), width = 1, fill = \"dodgerblue2\", colour = \"blue\") +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Uncontrolled first wave of COVID-19 in Thailand 1st to 24th March, 2020\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/load-data-1.png){width=672}\n:::\n:::\n\n\nWe'll re-use the functions from the last session\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Define model as per the previous session\n# Time window\ntimes <- seq(start_date, end_date, by = 1)\n\n# Model parameters\nparameters <- c(\n    R0 = 4,\n    latent_period = 5,\n    infectious_period = 6\n)\n\n# Initial conditions\nTotal_population <- 6.6e7 # Population of Thailand\nInitial_exposed <- 0\nInitial_infectious <- 20 # Initial infectious seed\nInitial_recovered <- 0\nInitial_susceptible <- Total_population - Initial_exposed - Initial_infectious - Initial_recovered\n\n# State variables\nstate <- c(\n    Susceptible = Initial_susceptible,\n    Exposed = Initial_exposed,\n    Infectious = Initial_infectious,\n    Recovered = Initial_recovered\n)\n\n\n# Model function\nCOVID.base <- function(t, state, parameters) {\n    with(as.list(c(state, parameters)), {\n        # Calculate the total population size\n        Total_population <- Susceptible + Exposed + Infectious + Recovered\n\n        # Calculate the average force of infection imposed on each susceptible individual\n        force_of_infection <- R0 * Infectious / (Total_population * infectious_period)\n\n        # Calculate the net (instantaneous) change in each state variable\n        Susceptible_change <- -force_of_infection * Susceptible\n        Exposed_change <- force_of_infection * Susceptible - Exposed / latent_period\n        Infectious_change <- Exposed / latent_period - Infectious / infectious_period\n        Recovered_change <- Infectious / infectious_period\n\n        # Return net changes as list\n        return(list(\n            c(\n                Susceptible_change,\n                Exposed_change,\n                Infectious_change,\n                Recovered_change\n            )\n        ))\n    })\n}\n\nsolve.base.model <- function(y_ = state,\n                             times_ = times,\n                             func. = COVID.base,\n                             parms_ = parameters) {\n    out <- ode(\n        y = y_,\n        times = as.numeric(times_ - times_[1]),\n        func = func.,\n        parms = parms_\n    )\n\n    # Calculate the prevalence, incidence and cumulative incidence (for comparison with data)\n    out <- as.data.frame(out) %>%\n        mutate(\n            Prevalence = Exposed + Infectious,\n            Incidence = Exposed / parms_[\"latent_period\"],\n            Cumulative_incidence = cumsum(Incidence) + Incidence[1],\n            Population = Susceptible + Exposed + Infectious + Recovered,\n            Date = times_\n        )\n\n    return(out)\n}\n```\n:::\n\n    \n## Solving the model for a specific set of parameters\nNow we use the model developed in the last session and solve the model for an initial set of parameters, an initial state and using dates from 7th - 26th March, 2020\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# now we use the model developed in the last session and solve it\n# for an initial set of parameters, an initial state and using times\n# from just 7th - 26th  of March 2020\nout_init <- solve.base.model(\n    y_ = state,\n    times_ = times,\n    func. = COVID.base,\n    parms_ = parameters\n)\n\n# Plot the filtered data and the first model \"guess\"\nggplot(uncontrolled_period) +\n    geom_col(aes(x = Date, y = Cases), width = 1, fill = \"dodgerblue2\", colour = \"blue\") +\n    geom_point(data = out_init, aes(x = Date, y = Incidence), size = 2, colour = \"firebrick2\") +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Thailand's First Wave, Jan-Jul 2020\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/initial-guess-1.png){width=672}\n:::\n:::\n\n  \n##   Calculating \"distance\" between the model and data\nWe  can use the model output, stored in `out_init` to assess model fit. We can use sum of squares of the difference between the data and the model.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# find the sum of the residuals squared for day 2 until day 24\nSSQ_initial <- sum((uncontrolled_period$Cases[-1] - out_init$Incidence[-1])^2)\nSSQ_initial\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24516.09\n```\n:::\n:::\n\n\nIf we were to minimise `SSQ_initial`, given our parameters $R_0$ and $I(t=0)$ we can argue that we have found our parameters of interest. \n\n## Transforming the parameters\nBefore launching into a minimisation of our SSQ function across all parameter space for the values of $R_0$ and $I(t=0)$, it would behoove us to pause and plan our search carefully. We will use the `optim()` function to minimize our function, but we should be cognisant of the fact that this function searches over all conceivable values of the parameters (i.e., from $-\\infty$ to $\\infty$).\n\nWhat are the feasible ranges of values for our two parameters: $R_0$ and $I(t=0)$?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nWith this in mind, it would be sensible to restrict our search to only those values of parameters that are feasible. Not only should this increase the speed of the algorithm (hopefully leading to the desired answer more quickly) but may also help us avoid any computational errors that might arise which will kill our search prematurely. For this purpose, we introduce a set of transformed parameters, that is parameters that exist in the full interval $-\\infty$ to $\\infty$, but can be mapped back to our feasible regions for our parameters.\n\n::: {.callout-note appearance=\"simple\"}\n## Transforming variables\n`optim()` is going to pass parameter values into our function on the interval $(-\\infty,\\infty)$. Is there a function that will take those parameter values and put them on the interval $[0,\\infty)$?\n\nIf $x \\in (-\\infty,\\infty)$, then $\\exp(x) \\in [0,\\infty)$. Therefore if we know $R_0$ and $I(t=0)$, we would use $\\log$ to transform back into the `optim()` space. If we want the opposite we can use $\\exp$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# transform the parameters\n# The search / optimization algorithm we employ searches over\n# the range (-Inf, Inf) for each variable. In our application\n# we are only interested in solutions in the range (0, Inf)\n# for R0 and I(0). Therefore, it\n# would be a good idea to apply a transformation to our\n# search space so that we are not wasting time exploring\n# infeasible regions of parameter space.\ninitial_transformed_parameters <- log(c(\n    \"R0\" = 4,\n    \"Initial_infectious\" = 20\n))\n```\n:::\n\n\nSo now we are in a position to define our function that we wish to optimise. The code is below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Fitting routine ####\n\n# We have selected R0 and I(0) as our free parameters\n# We need a function that accepts R0 and I(0) as arguments,\n# solves the model using these inputs, and calculates the\n# sum of squares of the data given these parameters\n\nSSQ_function <- function(transformed_parameters,\n                         data = uncontrolled_period$Cases[-1],\n                         state_base = state,\n                         times_ = times,\n                         func. = COVID.base,\n                         parms_base = parameters) {\n\n    # Untransform parameters\n    R0 <- exp(transformed_parameters[\"R0\"])\n    Initial_infectious <- exp(transformed_parameters[\"Initial_infectious\"])\n\n    # Calculate updated susceptible population\n    Initial_susceptible <- state_base[\"Susceptible\"] + state_base[\"Infectious\"] - Initial_infectious\n\n    # Overwrite baseline parameters with proposed parameters\n    parms_base[\"R0\"] <- R0\n    state_base[\"Susceptible\"] <- Initial_susceptible\n    state_base[\"Infectious\"] <- Initial_infectious\n\n    # Solve model with updated parameters\n    out <- solve.base.model(\n        state_base,\n        times_,\n        func.,\n        parms_base\n    )\n    return(\n        sum((uncontrolled_period$Cases[-1] - out$Incidence[-1])^2)\n    )\n}\n```\n:::\n\n    \nWe can check it works as intended:\n\n::: {.cell}\n\n```{.r .cell-code}\nSSQ_initial_from_function <- SSQ_function(initial_transformed_parameters)\nSSQ_initial_from_function\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24516.09\n```\n:::\n\n```{.r .cell-code}\nSSQ_initial\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24516.09\n```\n:::\n:::\n\n\nYay, we are getting the same result.\n\nWith our `SSQ_function` now set up, we can use the `optim` function to determine the optimal parameters, that is the parameter values that minimize the sum of squares.\n\nWe will use the Nelder-Mead search algorithm. This is a search algorithm that allows rapid exploration of the parameter space for the optimal parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Optimal parameters ####\n\n# Use the optim function to determine the parameters that\n# minimize the negative log-likelihood (i.e., maximize\n# the likelihood)\n# We will use the Nelder-Mead optimization solver\noptim_NM <- optim(\n    par = initial_transformed_parameters,\n    fn = SSQ_function,\n    control = list(maxit = 500),\n    method = \"Nelder-Mead\",\n    hessian = TRUE\n)\n```\n:::\n\nOnce the search is complete, inspect the solution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for convergence (always code 0 for NM)\noptim_NM$convergence # 0 - converged; 1 - failed to converge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# Inspect solution\noptim_NM$par\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                R0 Initial_infectious \n          1.094083           4.302341 \n```\n:::\n:::\n\nWhat are the initial values of the two fitted parameters?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nThe results are a little surprising. However, remember that our search was conducted with reference to the transformed parameters; in order to retrieve the true optimal parameter values we must invert the transformation:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Back-transform parameters\noptimum_parameters <- exp(optim_NM$par)\noptimum_parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                R0 Initial_infectious \n          2.986444          73.872554 \n```\n:::\n:::\n\n\nBut have we improved our SSQ from the initial guess we had?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect the optimal negative log-likelihood\noptimum_NLL <- optim_NM$value\noptimum_NLL\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15100\n```\n:::\n:::\n\nHow does the SSQ score at the optimal value compare with our initial SSQ score (`SSQ_initial`)?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nSatisfied that we have found the optimal solution, let's now solve the model using these parameter values and compare the solution with our observed data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Optimal model solution ####\n\n# Create the optimal parameter and state vectors\noptimal_parameters <- parameters\noptimal_parameters[\"R0\"] <- optimum_parameters[\"R0\"]\n\noptimal_initial_state <- state\noptimal_initial_state[\"Susceptible\"] <- state[\"Susceptible\"] + state[\"Infectious\"] - optimum_parameters[\"Initial_infectious\"]\noptimal_initial_state[\"Infectious\"] <- optimum_parameters[\"Initial_infectious\"]\n\n# Solve the model given the optimal parameters and initial conditions\noptimal_solution <- solve.base.model(\n    y_ = optimal_initial_state,\n    times_ = times,\n    func. = COVID.base,\n    parms = optimal_parameters\n)\n\n\n# Plot the optimal solution\nggplot(first_wave) +\n    geom_col(aes(x = Date, y = Cases), width = 1, fill = \"dodgerblue2\", colour = \"blue\") +\n    geom_point(data = optimal_solution, aes(x = Date, y = Incidence), size = 2, colour = \"firebrick2\") +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Fit to unmitigated period\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/SSQ-optimal-fit-with-data-1.png){width=672}\n:::\n:::\n\n\nHow does this fit compare to the observed data?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\n# Likelihood Functions\n\nIn the previous section you learnt of one way to measure how far the model results are from data. When you minimise SSQ, you are performing a [Least Squares Fit](https://en.wikipedia.org/wiki/Least_squares). This is drawing the line of best fit through all of your data points (obviously you might minimise your SSQ further by changing your choice of line, but then we run into overfitting problems). \n\nIn the literature, a much more common way to fit parameters is to find the [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). But what is the likelihood?\n\n::: {.callout-note appearance=\"simple\"}\n\n## Definition: Likelihood function\n\nThe likelihood is a function that describes how **likely** your observed data is given your model and its parameters. \n\n:::\n\nWhy would we want to use the likelihood over SSQ?\n\nYou might have noticed that the model fit in the last section was not... very good. It appears to just go through the middle of the data and not describe any one point very well. This is because our model wanted to minimise the sum of squared error! In reality, it makes more sense to consider that each data point is an observation around which there is uncertainty that can be represented by a statistical distribution.\n\nSo now the question is *how do we determine what our likelihood function should be?*\n\nThis is not easy to answer in general, but what we can teach is easy ways to construct likelihood functions.\n\nAs the likelihood is the probability of data given your parameters, it is typically defined as,\n\n$$\np(\\vec{D}|\\vec{\\theta})\n$$\n\nwhere $\\vec{D}$ is the vector of all data points and $\\vec{\\theta}$ is a vector containing the model parameters. In words, we could say that the probability of observing our data given our parameters is the same as observing the first data point and the second data point and the third data point... so on given our parameters. So let us express our likelihood as,\n\n$$\np(\\vec{D}|\\vec{\\theta}) = \\Pi_{i=1}^N p(D_i|\\vec{\\theta}),\n$$\n\nwhere $D_i$ is the $i$th data point in the vector of data. Let us look at the figure below for a visualisation of the likelihood.\n\n::: {.panel-tabset}\n## Poisson Distribution\n\n::: {.cell}\n::: {.cell-output-display}\n![The likelihood assigns the probability of observing data given your underlying model and model parameters.](session5_fitting_models_in_r_files/figure-html/Likelihood-1.png){width=672}\n:::\n:::\n\n\n## Negative Binomial\n\n::: {.cell}\n::: {.cell-output-display}\n![Changing the assumed distribution that the data is from will change the likelihood. We can see that by assuming that each data point is negative binomial we get more variance in our distribution.](session5_fitting_models_in_r_files/figure-html/NegativeBinomialLikelihood-1.png){width=672}\n:::\n:::\n\n:::\n\nWhen using the likelihood, it is much more common for people to work with log-likelihood. The reason for this, is that with a large number of data points you very quickly run out of computational precision (each probability will be less than one and computers cannot store super small numbers well). From now on we will work with a log-likelihood, which means all products will become sums. That is, \n\n$$\n\\log p(\\vec{D}|\\vec{\\theta}) = \\sum_{i=1}^N \\log p(D_i|\\vec{\\theta}),\n$$\n\n\n## Covid-19 Likelihood function\nLet us calculate the *likelihood* of our observed data given our COVID model (which of course is specific to the current parameter values that we have used). To do this, a common practice is to assume that the observed daily incidence (stored in the dataframe `uncontrolled_period`) is a Poisson-distributed random variable with mean given by our model-predicted incidence (stored in the dataframe `out_init`). Under this model the (log-)likelihood of each daily observation can be calculated using the dpois() function; and the total (log-)likelihood for all of the observations is simply the sum of all these:\n\n$$\n\\log p(\\text{Cases}|\\vec{\\theta}) = \\sum_{i=1}^N \\log \\text{Poisson}(c_i|\\mu = \\text{Model}(\\theta,i)),\n$$\n\nwhere Cases is the vector containing our case data, $c_i$ is the number of cases on the $i$th day and Model$(\\theta,i)$ is the model output given our parameters on day $i$.\n\n\n<!-- ```{r log-likelihood}\n# Calculate the negative log-likelihood for our initial parameter values, assuming a Poisson residual distribution\npoisson_negative_log_likelihood_initial <- -sum(dpois(uncontrolled_period$Cases[-1],\n    out_init$Incidence[-1],\n    log = TRUE\n))\npoisson_negative_log_likelihood_initial\n``` -->\n  \nThere are many possible distributions that can be used to develop the likelihood. You may wish to investigate some of the density distributions available in R, which you can do by typing `?distributions` in your console.\n\nAnother distribution, which allows us to account for high variability in the data is the negative binomial distribution. We could use the output from our last function call to calculate the likelihood of our observed data given our COVID model using `dnbinom()` function in much the same way as the Poisson distribution. Those with a particular interest in statistics and probability distributions may wish to experiment and to consider the pros and cons of different choices. For the remainder of this exercise, however, we will continue with the Poisson distribution which has the attribute of having only one parameter to estimate, the mean.\n\n:::{.callout-tip}\n## Changing distributions\nThe decision on what distribution to use within your likelihood function is a *Model Choice*. Sometimes, multiple distributions may suit your need. Be aware that each distribution has different assumptions. \n:::\n\n# Parameter estimation with maximum likelihood\nWith our likelihood function defined mathematically, let us now start thinking about how to set it up in R. We again want to use the `optim` function to determine the optimal parameters, however, `optim()` will solve for the minimum as opposed to the maximum (we want the maximum likelihood). Because of this, we shall put a minus sign in front of our likelihood, that way finding the minimum of the new function will actually calculate the maximum that we want!\n\nSo now we are equipped to fit our likelihood function, using code like below:\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Fitting routine ####\n\n# We have selected R0 and I(0) as our free parameters\n# We need a function that accepts R0 and I(0) as arguments,\n# solves the model using these inputs, and calculates the\n# negative log-likelihood of the data given these parameters\n\n\nnegative_log_likelihood <- function(transformed_parameters,\n                                    data = uncontrolled_period$Cases[-1],\n                                    state_base = state,\n                                    times_ = times,\n                                    func. = COVID.base,\n                                    parms_base = parameters) {\n\n    # Untransform parameters\n    R0 <- exp(transformed_parameters[\"R0\"])\n    Initial_infectious <- exp(transformed_parameters[\"Initial_infectious\"])\n    # Calculate updated susceptible population\n    Initial_susceptible <- state_base[\"Susceptible\"] + state_base[\"Infectious\"] - Initial_infectious\n\n    # Overwrite baseline parameters with proposed parameters\n    parms_base[\"R0\"] <- R0\n    state_base[\"Susceptible\"] <- Initial_susceptible\n    state_base[\"Infectious\"] <- Initial_infectious\n\n    # Solve model with updated parameters\n    out <- solve.base.model(\n        state_base,\n        times_,\n        func.,\n        parms_base\n    )\n\n    return(-sum(dpois(\n        x = data,\n        lambda = out$Incidence[-1],\n        log = TRUE\n    )))\n}\n```\n:::\n\n<!-- Lets check it works as intended again : -->\n<!-- ```{r fit-NLL-check}\nnegative_log_likelihood_from_function <- negative_log_likelihood(initial_transformed_parameters)\nnegative_log_likelihood_from_function\npoisson_negative_log_likelihood_initial\nprint(initial_transformed_parameters)\n``` -->\n\nLet us now calculate our optimal parameters!\n\n::: {.cell}\n\n```{.r .cell-code}\n#### Optimal parameters ####\n\n# Use the optim function to determine the parameters that\n# minimize the negative log-likelihood (i.e., maximize\n# the likelihood)\n# We will use the Nelder-Mead optimization solver\noptim_NM <- optim(\n    par = initial_transformed_parameters,\n    fn = negative_log_likelihood,\n    control = list(maxit = 500),\n    method = \"Nelder-Mead\",\n    hessian = TRUE\n)\n```\n:::\nOnce the search is complete, inspect the solution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for convergence (always code 0 for NM)\noptim_NM$convergence # 0 - converged; 1 - failed to converge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# Inspect solution\noptim_NM$par\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                R0 Initial_infectious \n          1.378931           3.413004 \n```\n:::\n:::\n\nWhat are the initial values of the two fitted parameters?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\nThe results are a little surprising. However, remember that our search was conducted with reference to the transformed parameters; in order to retrieve the true optimal parameter values we must invert the transformation:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Back-transform parameters\noptimum_parameters <- exp(optim_NM$par)\noptimum_parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                R0 Initial_infectious \n          3.970654          30.356305 \n```\n:::\n:::\n\n\nAre there any differences between these results and the fitted parameters in the previous section?\n<textarea name='Text1' cols='127' rows='2'></textarea>\nWhy might this be?\n<textarea name='Text1' cols='127' rows='2'></textarea>\nIf we chose a different distribution, could we get a different answer?\n<textarea name='Text1' cols='127' rows='2'></textarea>\n\n\n## Bonus code \nCopy and paste this code to perform a grid search - getting the negative log likelihood across a sequence of each parameter value.\n\n::: {.cell hash='session5_fitting_models_in_r_cache/html/grid-search_e45f3f7ec31ed17dc7310e94327944dc'}\n\n```{.r .cell-code}\n# Specify search grid\nR0_vec <- seq(from = 0.1, to = 10, by = 0.1)\ninitial_infectious_vec <- seq(from = 1, to = 50, by = 1)\n\n# the function that will perform grid search\n\nnll.grid <- function(R0_vec, initial_infectious_vec) {\n    parameter_grid <- expand.grid(\n        R0 = R0_vec,\n        initial_infectious = initial_infectious_vec\n    )\n    nll <- data.frame(matrix(nrow = nrow(parameter_grid), ncol = 1))\n    colnames(nll) <- \"NLL\"\n    for (row_of_params in seq(1, dim(parameter_grid)[1])) {\n        transformed_params <- log(c(\n            \"R0\" = parameter_grid[row_of_params, 1],\n            \"Initial_infectious\" = parameter_grid[row_of_params, 2]\n        ))\n        nll[row_of_params, ] <- negative_log_likelihood(transformed_params)\n    }\n    return(data.frame(\n        Reproduction_number = parameter_grid[\"R0\"],\n        Initial_infectious = parameter_grid[\"initial_infectious\"],\n        nll\n    ))\n}\n\n# call the function that will calculate likelihood surface\nnll_grid_results <- nll.grid(R0_vec, initial_infectious_vec)\n\n\n# Plot likelihood surface\nnll_plot <- nll_grid_results %>%\n    ggplot2::ggplot(aes(\n        x = R0,\n        y = initial_infectious,\n        fill = NLL,\n        z = NLL\n    )) +\n    geom_tile() +\n    geom_contour(breaks = c(100, 110, 120, 130, 200, 500, 1000, 4000)) +\n    ylab(\"I(0)\") +\n    xlab(\"R0\") +\n    ggtitle(\"Likelihood surface\") +\n    theme_bw()\n\n# Extract optimum from grid search\ngrid_optimum <- nll_grid_results[which.min(nll_grid_results$NLL), ]\ngrid_optimum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     R0 initial_infectious      NLL\n2940  4                 30 144.9431\n```\n:::\n\n```{.r .cell-code}\n# Superimpose optimum on likelihood surface plot\nnll_plot <- nll_plot +\n    geom_point(aes(\n        x = grid_optimum$R0,\n        y = grid_optimum$initial_infectious\n    ),\n    colour = \"red\",\n    )\n# Superimpose optimal point to negative-log-likelihood plot\nnll_plot <- nll_plot + geom_point(aes(\n    x = optimum_parameters[\"R0\"],\n    y = optimum_parameters[\"Initial_infectious\"]\n),\ncolour = \"yellow\",\nsize = 1\n)\n\nprint(nll_plot)\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/grid-search-1.png){width=672}\n:::\n:::\n\n\nWhy might a grid search not be the best option for finding optimum parameter values ?\n<textarea name='Text1' cols='127' rows='4'></textarea>\n\n# Calculating confidence intervals\nWe are now going to put some confidence intervals around the data. This is commonly done but has some problems that relate to model assumptions. For example, if we use the Poisson distribution for estimating parameters, it assumes the spread of the data has a variance equal to the mean. The Negative Binomial distribution allows for greater data variance, but we have one extra parameter to estimate - the variance of the negative binomial distribution itself. Below is some code to examine the confidence intervals around your data with the caveats given above.\n\nFirstly, calculate the Poisson distribution centred on the optimised model solution. Find the inter-quartile range and 95% confidence interval of this distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the observational confidence intervals\noptimal_solution <- optimal_solution %>%\n    mutate(\n        lower50 = qpois(p = 0.25, lambda = Incidence), # 50% confidence interval (i.e., 25 - 75th centiles)\n        upper50 = qpois(p = 0.75, lambda = Incidence),\n        lower95 = qpois(p = 0.025, lambda = Incidence), # 95% confidence interval (i.e., 2.5 - 97.5th centiles)\n        upper95 = qpois(p = 0.975, lambda = Incidence)\n    )\n\n# Plot confidence intervals as ribbons around the central estimates\nggplot(first_wave) +\n    geom_col(aes(x = Date, y = Cases), width = 1.0, fill = \"dodgerblue2\", colour = \"blue\") +\n    geom_ribbon(data = optimal_solution[-1, ], aes(x = Date, ymin = lower50, ymax = upper50), fill = \"firebrick2\", colour = \"firebrick2\", alpha = 0.8) +\n    geom_ribbon(data = optimal_solution[-1, ], aes(x = Date, ymin = lower95, ymax = upper95), fill = \"firebrick2\", colour = \"firebrick2\", alpha = 0.5) +\n    ylab(\"Daily cases\") +\n    xlab(\"\") +\n    ggtitle(\"Fit to unmitigated period (with observational uncertainty)\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](session5_fitting_models_in_r_files/figure-html/observation-uncertainty-1.png){width=672}\n:::\n:::\n\n\nMore important than confidence intervals around the data are confidence intervals around the estimated parameters. We found an optimum value for the estimated parameters using the R inbuilt optim function, but this function can do much more.\n\nIn addition to the point estimates for each of the parameters, we can also use the results of the optimization search to calculate confidence intervals for them too. The idea is to use the curvature of the likelihood surface to determine the range of parameter values that exceed a particular likelihood score (pointer likelihood surfaces should lead to more smaller confidence intervals). The curvature values are stored in what is known as the Hessian matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim_NM$hessian\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          R0 Initial_infectious\nR0                 10561.786          3159.1981\nInitial_infectious  3159.198           994.9545\n```\n:::\n:::\n\n\nThe covariance matrix is the inverse of this matrix, and be calculated using:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Covariance matrix\ncovar_matrix <- solve(optim_NM$hessian)\ncovar_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             R0 Initial_infectious\nR0                  0.001884512       -0.005983737\nInitial_infectious -0.005983737        0.020004746\n```\n:::\n:::\n\n\nUsing the covariance matrix, we can next calculate the standard errors for each parameter by taking the square root of the diagonal elements of the covariance matrix, and then adding and subtracting these to the central estimates to obtain the 95% confidence intervals:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now use the covariance matrix to generate standard errors for each parameter\nstd_errors <- sqrt(diag(covar_matrix))\nestimates_transformed <- data.frame(\n    estimate = optim_NM$par,\n    std_error = std_errors\n) %>%\n    mutate(\n        lower95CI = estimate - 1.96 * std_error,\n        upper95CI = estimate + 1.96 * std_error\n    )\nestimates_transformed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   estimate  std_error lower95CI upper95CI\nR0                 1.378931 0.04341097  1.293845  1.464016\nInitial_infectious 3.413004 0.14143813  3.135785  3.690223\n```\n:::\n\n```{.r .cell-code}\n# Back-transform estimates and confidence intervals\nestimates <- exp(estimates_transformed[, -2])\nestimates\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    estimate lower95CI upper95CI\nR0                  3.970654  3.646783  4.323288\nInitial_infectious 30.356305 23.006700 40.053777\n```\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}