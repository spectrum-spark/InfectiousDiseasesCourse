# Initial conditions
Total_population <- 6.6e7
Initial_exposed <- 0
Initial_infected <- 20
Initial_recovered <- 0
Initial_susceptible <- Total_population - Initial_exposed - Initial_infected - Initial_recovered
# Compartments
state <- c(
Susceptible = Initial_susceptible,
Exposed = Initial_exposed,
Infectious = Initial_infected,
Recovered = Initial_recovered
)
# Time window
start_date <- as.Date("2020-03-07")
end_date <- as.Date("2020-03-26")
times <- seq(start_date, end_date, by = 1)
# Model function
COVID.base <- function(t, state, parameters) {
with(as.list(c(state, parameters)), {
# Calculate the total population size
Total_population <- Susceptible + Exposed + Infectious + Recovered
# Calculate the average force of infection imposed on each susceptible individual
force_of_infection <- R0 * Infectious / (Total_population * infectious_period) # i.e. the infection rate
# Calculate the net (instantaneous) change in each state variable
Susceptible_change <- -force_of_infection * Susceptible # dS/dt
Exposed_change <- force_of_infection * Susceptible - Exposed / latent_period  # dE/dt
Infectious_change <- Exposed / latent_period - Infectious / infectious_period  # dI/dt
Recovered_change <- Infectious / infectious_period # dR/dt
# Return net changes as list
return(list(
c(
Susceptible_change,
Exposed_change,
Infectious_change,
Recovered_change
)
))
})
}
# Solve model
out <- ode(y = state, times = as.numeric(times - times[1]), func = COVID.base, parms = parameters)
# Plot solution
par(mar = c(1, 1, 1, 1)) # reduce the margins of the plot in order to fit it in the panel
plot(out)
tidy_base_model <- function(ode_output, times, parameters) {
as.data.frame(ode_output) %>%
mutate(
Prevalence = Exposed + Infectious,
Incidence = Exposed / parameters["latent_period"],
Cumulative_incidence = cumsum(Incidence),
Population = Susceptible + Exposed + Infectious + Recovered,
Date = times
)
}
tidied_output <- tidy_base_model(out, times, parameters)
ggplot() +
geom_line(aes(x = Date, y = Prevalence), data = tidied_output, linewidth = 1, colour = "firebrick") +
labs(x="Date", y="Modelled Prevalence") +
ggtitle("COVID19's First Wave, Jan-Jul 2020") +
theme_bw()
ggplot() +
geom_line(aes(x = Date, y = Population), data = tidied_output, linewidth = 1, colour = "firebrick") +
ggtitle("COVID19's First Wave, Jan-Jul 2020") +
labs(x="Date", y="Total Population") +
ggtitle("COVID19's First Wave, Jan-Jul 2020") +
theme_bw()
# Compare the solution with the data
# Incidence
ggplot(first_wave) +
geom_col(aes(x=Date, y=Cases), width=1, fill=NA, colour="black") +
geom_line(data=tidied_output, aes(x=Date, y=Incidence), linewidth=1, colour="firebrick") +
ylab("Daily cases") +
xlab("") +
ggtitle("Model-predicted incidence") +
theme_bw()
# Cumulative incidence
# first calculate data cumulative cases from our date of interest
cum_cases_reduced_dates = cumsum(first_wave$Cases[which(first_wave$Date>=as.Date(start_date,format = "%Y-%m-%d"))])
tidied_output$Data_cum_cases = cum_cases_reduced_dates[1:20]
ggplot(tidied_output) +
geom_point(aes(x=Date, y=Data_cum_cases), size=2, colour="skyblue") +
geom_line(data=tidied_output, aes(x=Date, y=Cumulative_incidence), linewidth=1, colour="firebrick") +
ylab("Cumulative cases") +
xlab("") +
ggtitle("Model-predicted cumulative incidence") +
theme_bw()
start_date = as.Date("2020-03-07")
end_date = as.Date("2020-04-26")
times_long = seq(start_date, end_date, by=1)
long_out <- ode(y = state, times = as.numeric(times_long - times_long[1]), func = COVID.base, parms = parameters)
tidied_long <- tidy_base_model(long_out, times_long, parameters)
knitr::opts_chunk$set(
fig.align = "center"
)
#| message: false
#| warning: false
# SPARKLE Modelling Short course
#########################
## FITTING MODELS IN R ##
#########################
if (!require(pacman)) install.packages("pacman")
library(pacman)
pacman::p_load(dplyr, deSolve, ggplot2)
# Data imports and filtering
first_wave <- read.csv("first_wave_TH.csv", colClasses = c("Date", "numeric", "numeric"))
# Time window
start_date <- as.Date("2020-03-07")
end_date <- as.Date("2020-03-26")
# Filter data to capture period prior to interventions. This has already been done in the last session.
uncontrolled_period <- first_wave %>%
filter(Date >= start_date, Date <= end_date)
# Plot the filtered data
ggplot(uncontrolled_period) +
geom_col(aes(x = Date, y = Cases), width = 1, fill = "dodgerblue2", colour = "blue") +
ylab("Daily cases") +
xlab("") +
ggtitle("Uncontrolled first wave of COVID-19 1st to 24th March, 2020") +
theme_bw()
## Define model as per the previous session
# Time window
times <- seq(start_date, end_date, by = 1)
# Model parameters
parameters <- c(
R0 = 4,
latent_period = 5,
infectious_period = 6
)
# Initial conditions
Total_population <- 6.6e7 # Population
Initial_exposed <- 0
Initial_infectious <- 20 # Initial infectious seed
Initial_recovered <- 0
Initial_susceptible <- Total_population - Initial_exposed - Initial_infectious - Initial_recovered
# Compartments
state <- c(
Susceptible = Initial_susceptible,
Exposed = Initial_exposed,
Infectious = Initial_infectious,
Recovered = Initial_recovered
)
# Model function
COVID.base <- function(t, state, parameters) {
with(as.list(c(state, parameters)), {
# Calculate the total population size
Total_population <- Susceptible + Exposed + Infectious + Recovered
# Calculate the average force of infection imposed on each susceptible individual
force_of_infection <- R0 * Infectious / (Total_population * infectious_period)
# Calculate the net (instantaneous) change in each state variable
Susceptible_change <- -force_of_infection * Susceptible
Exposed_change <- force_of_infection * Susceptible - Exposed / latent_period
Infectious_change <- Exposed / latent_period - Infectious / infectious_period
Recovered_change <- Infectious / infectious_period
# Return net changes as list
return(list(
c(
Susceptible_change,
Exposed_change,
Infectious_change,
Recovered_change
)
))
})
}
solve.base.model <- function(y_ = state,
times_ = times,
func. = COVID.base,
parms_ = parameters) {
out <- ode(
y = y_,
times = as.numeric(times_ - times_[1]),
func = func.,
parms = parms_
)
# Calculate the prevalence, incidence and cumulative incidence (for comparison with data)
out <- as.data.frame(out) %>%
mutate(
Prevalence = Exposed + Infectious,
Incidence = Exposed * (1/parms_["latent_period"]),
Cumulative_incidence = cumsum(Incidence) + Incidence[1],
Population = Susceptible + Exposed + Infectious + Recovered,
Date = times_
)
return(out)
}
# now we use the model developed in the last session and solve it
# for an initial set of parameters, an initial state and using times
# from just 7th - 26th  of March 2020
out_init <- solve.base.model(
y_ = state,
times_ = times,
func. = COVID.base,
parms_ = parameters
)
# Plot the filtered data and the first model "guess"
ggplot(uncontrolled_period) +
geom_col(aes(x = Date, y = Cases), width = 1, fill = "dodgerblue2", colour = "blue") +
geom_point(data = out_init, aes(x = Date, y = Incidence), size = 2, colour = "firebrick2") +
ylab("Daily cases") +
xlab("") +
ggtitle("COVID19's First Wave, Jan-Jul 2020") +
theme_bw()
# find the sum of the residuals squared for day 2 until day 24
SSQ_initial <- sum((uncontrolled_period$Cases[-1] - out_init$Incidence[-1])^2)
SSQ_initial
# transform the parameters
# The search / optimization algorithm we employ searches over
# the range (-Inf, Inf) for each variable. In our application
# we are only interested in solutions in the range (0, Inf)
# for R0 and I(0). Therefore, it
# would be a good idea to apply a transformation to our
# search space so that we are not wasting time exploring
# infeasible regions of parameter space.
initial_transformed_parameters <- log(c(
"R0" = 4,
"Initial_infectious" = 20
))
#### Fitting routine ####
# We have selected R0 and I(0) as our free parameters
# We need a function that accepts R0 and I(0) as arguments,
# solves the model using these inputs, and calculates the
# sum of squares of the data given these parameters
SSQ_function <- function(transformed_parameters,
data = uncontrolled_period$Cases[-1],
state_base = state,
times_ = times,
func. = COVID.base,
parms_base = parameters) {
# Untransform parameters
R0 <- exp(transformed_parameters["R0"])
Initial_infectious <- exp(transformed_parameters["Initial_infectious"])
# Calculate updated susceptible population
Initial_susceptible <- state_base["Susceptible"] + state_base["Infectious"] - Initial_infectious
# Overwrite baseline parameters with proposed parameters
parms_base["R0"] <- R0
state_base["Susceptible"] <- Initial_susceptible
state_base["Infectious"] <- Initial_infectious
# Solve model with updated parameters
out <- solve.base.model(
state_base,
times_,
func.,
parms_base
)
return(
sum((uncontrolled_period$Cases[-1] - out$Incidence[-1])^2)
)
}
SSQ_initial_from_function <- SSQ_function(initial_transformed_parameters)
SSQ_initial_from_function
SSQ_initial
#### Optimal parameters ####
# Use the optim function to determine the parameters that
# minimize the negative log-likelihood (i.e., maximize
# the likelihood)
# We will use the Nelder-Mead optimization solver
optim_NM <- optim(
par = initial_transformed_parameters,
fn = SSQ_function,
control = list(maxit = 500),
method = "Nelder-Mead",
hessian = TRUE
)
# Check for convergence (always code 0 for NM)
optim_NM$convergence # 0 - converged; 1 - failed to converge
# Inspect solution
optim_NM$par
# Back-transform parameters
optimum_parameters <- exp(optim_NM$par)
optimum_parameters
# Inspect the optimal sum of squared residuals
optimum_SSQ <- optim_NM$value
optimum_SSQ
#### Optimal model solution ####
# Create the optimal parameter and state vectors
optimal_parameters <- parameters
optimal_parameters["R0"] <- optimum_parameters["R0"]
optimal_initial_state <- state
optimal_initial_state["Susceptible"] <- state["Susceptible"] + state["Infectious"] - optimum_parameters["Initial_infectious"]
optimal_initial_state["Infectious"] <- optimum_parameters["Initial_infectious"]
# Solve the model given the optimal parameters and initial conditions
optimal_solution <- solve.base.model(
y_ = optimal_initial_state,
times_ = times,
func. = COVID.base,
parms = optimal_parameters
)
# Plot the optimal solution
ggplot(first_wave) +
geom_col(aes(x = Date, y = Cases), width = 1, fill = "dodgerblue2", colour = "blue") +
geom_line(data = optimal_solution, aes(x = Date, y = Incidence), size = 2, colour = "firebrick2") +
ylab("Daily cases") +
xlab("") +
ggtitle("Fit to unmitigated period") +
theme_bw()
ggplot(uncontrolled_period) +
geom_col(aes(x = Date, y = Cases), width = 1, fill = "dodgerblue2", colour = "blue") +
geom_line(data = optimal_solution, aes(x = Date, y = Incidence), size = 2, colour = "firebrick2") +
ylab("Daily cases") +
xlab("") +
ggtitle("Fit to unmitigated period") +
theme_bw()
#| label: Likelihood
#| fig-cap: "The likelihood assigns the probability of observing data given your underlying model and model parameters."
#| echo: false
#| message: false
set.seed(1)
library(magrittr)
library(dplyr)
library(ggplot2)
sir_data <- tibble(t = c(25, 50, 75, 100), y = exp(0.05 * t)) %>%
rowwise() %>%
mutate(rand_y = rpois(1, y))
sir_data <- tibble(t = c(25, 50, 75, 100), y = exp(0.05 * t)) %>%
rowwise() %>%
mutate(rand_y = rnbinom(1, mu = y, size = 5))
# # SIR contains the answer
sir <- tibble(t = 0:110, y = exp(0.05 * t))
# Construct the fun figure
distribution_data <- tibble()
times_gg <- c(25, 50, 75, 100)
for (time in times_gg) {
distribution_data <- distribution_data %>% bind_rows(., tibble(t = time, height = 0:300, width = dpois(height, (sir %>% filter(t == time))$y), width2 = dnbinom(height, mu = (sir %>% filter(t == time))$y, size = 10)))
}
ggplot() +
geom_ribbon(
data = distribution_data %>% filter(width >= 1e-10),
aes(xmin = t, xmax = t - 90 * width, y = height, group = t, fill = "Model Fit", color = "Model Fit"),
outline.type = "full", alpha = 0.4
) +
geom_point(
data = sir_data,
aes(x = t, y = rand_y, color = "Data", fill = "Data")
) +
geom_line(data = sir, aes(y = y, x = t)) +
scale_y_continuous("Daily Cases", limits = c(NA, 300)) +
scale_x_continuous("Day") +
ggtitle("Visualisation of the Likelihood") +
scale_fill_manual(name = "Legend", values = c("Model Fit" = "blue", "Data" = "red")) +
scale_color_manual(name = "Legend", values = c("Data" = "red", "Model Fit" = "black")) +
theme(legend.position = "bottom")
#| label: NegativeBinomialLikelihood
#| fig-cap: "Changing the assumed distribution that the data is from will change the likelihood. We can see that by assuming that each data point is negative binomial we get more variance in our distribution."
#| echo: false
#| message: false
ggplot() +
geom_ribbon(
data = distribution_data %>% filter(width >= 1e-10),
aes(xmin = t, xmax = t - 90 * width2, y = height, group = t, fill = "Model Fit", color = "Model Fit"),
outline.type = "full", alpha = 0.4
) +
geom_point(
data = sir_data,
aes(x = t, y = rand_y, color = "Data", fill = "Data")
) +
geom_line(data = sir, aes(y = y, x = t)) +
scale_y_continuous("Daily Cases", limits = c(NA, 300)) +
scale_x_continuous("Day") +
ggtitle("Visualisation of the Likelihood") +
scale_fill_manual(name = "Legend", values = c("Model Fit" = "blue", "Data" = "red")) +
scale_color_manual(name = "Legend", values = c("Data" = "red", "Model Fit" = "black")) +
theme(legend.position = "bottom")
#### Fitting routine ####
# We have selected R0 and I(0) as our free parameters
# We need a function that accepts R0 and I(0) as arguments,
# solves the model using these inputs, and calculates the
# negative log-likelihood of the data given these parameters
negative_log_likelihood <- function(transformed_parameters,
data = uncontrolled_period$Cases[-1],
state_base = state,
times_ = times,
func. = COVID.base,
parms_base = parameters) {
# Untransform parameters
R0 <- exp(transformed_parameters["R0"])
Initial_infectious <- exp(transformed_parameters["Initial_infectious"])
# Calculate updated susceptible population
Initial_susceptible <- state_base["Susceptible"] + state_base["Infectious"] - Initial_infectious
# Overwrite baseline parameters with proposed parameters
parms_base["R0"] <- R0
state_base["Susceptible"] <- Initial_susceptible
state_base["Infectious"] <- Initial_infectious
# Solve model with updated parameters
out <- solve.base.model(
state_base,
times_,
func.,
parms_base
)
return(-sum(dpois(
x = data,
lambda = out$Incidence[-1],
log = TRUE
)))
}
#### Optimal parameters ####
# Use the optim function to determine the parameters that
# minimize the negative log-likelihood (i.e., maximize
# the likelihood)
# We will use the Nelder-Mead optimization solver
optim_NM <- optim(
par = initial_transformed_parameters,
fn = negative_log_likelihood,
control = list(maxit = 500),
method = "Nelder-Mead",
hessian = TRUE
)
# Check for convergence (always code 0 for NM)
optim_NM$convergence # 0 - converged; 1 - failed to converge
# Inspect solution
optim_NM$par
# Back-transform parameters
optimum_parameters <- exp(optim_NM$par)
optimum_parameters
#| cache: TRUE
#| warning: false
# Specify search grid
R0_vec <- seq(from = 0.1, to = 10, by = 0.1)
initial_infectious_vec <- seq(from = 1, to = 50, by = 1)
# the function that will perform grid search
nll.grid <- function(R0_vec, initial_infectious_vec) {
parameter_grid <- expand.grid(
R0 = R0_vec,
initial_infectious = initial_infectious_vec
)
nll <- data.frame(matrix(nrow = nrow(parameter_grid), ncol = 1))
colnames(nll) <- "NLL"
for (row_of_params in seq(1, dim(parameter_grid)[1])) {
transformed_params <- log(c(
"R0" = parameter_grid[row_of_params, 1],
"Initial_infectious" = parameter_grid[row_of_params, 2]
))
nll[row_of_params, ] <- negative_log_likelihood(transformed_params)
}
return(data.frame(
Reproduction_number = parameter_grid["R0"],
Initial_infectious = parameter_grid["initial_infectious"],
nll
))
}
# call the function that will calculate likelihood surface
nll_grid_results <- nll.grid(R0_vec, initial_infectious_vec)
# Plot likelihood surface
nll_plot <- nll_grid_results %>%
ggplot2::ggplot(aes(
x = R0,
y = initial_infectious,
fill = NLL,
z = NLL
)) +
geom_tile() +
geom_contour(breaks = c(100, 110, 120, 130, 200, 500, 1000, 4000)) +
ylab("I(0)") +
xlab("R0") +
ggtitle("Likelihood surface") +
theme_bw()
# Extract optimum from grid search
grid_optimum <- nll_grid_results[which.min(nll_grid_results$NLL), ]
grid_optimum
# Superimpose optimum on likelihood surface plot
nll_plot <- nll_plot +
geom_point(aes(
x = grid_optimum$R0,
y = grid_optimum$initial_infectious
),
colour = "red",
)
# Superimpose optimal point to negative-log-likelihood plot
nll_plot <- nll_plot + geom_point(aes(
x = optimum_parameters["R0"],
y = optimum_parameters["Initial_infectious"]
),
colour = "yellow",
size = 1
)
print(nll_plot)
# Calculate the observational confidence intervals
optimal_solution <- optimal_solution %>%
mutate(
lower50 = qpois(p = 0.25, lambda = Incidence), # 50% confidence interval (i.e., 25 - 75th centiles)
upper50 = qpois(p = 0.75, lambda = Incidence),
lower95 = qpois(p = 0.025, lambda = Incidence), # 95% confidence interval (i.e., 2.5 - 97.5th centiles)
upper95 = qpois(p = 0.975, lambda = Incidence)
)
# Plot confidence intervals as ribbons around the central estimates
ggplot(first_wave) +
geom_col(aes(x = Date, y = Cases), width = 1.0, fill = "dodgerblue2", colour = "blue") +
geom_ribbon(data = optimal_solution[-1, ], aes(x = Date, ymin = lower50, ymax = upper50), fill = "firebrick2", colour = "firebrick2", alpha = 0.8) +
geom_ribbon(data = optimal_solution[-1, ], aes(x = Date, ymin = lower95, ymax = upper95), fill = "firebrick2", colour = "firebrick2", alpha = 0.5) +
ylab("Daily cases") +
xlab("") +
ggtitle("Fit to unmitigated period (with observational uncertainty)") +
theme_bw()
optim_NM$hessian
# Covariance matrix
covar_matrix <- solve(optim_NM$hessian)
covar_matrix
# Now use the covariance matrix to generate standard errors for each parameter
std_errors <- sqrt(diag(covar_matrix))
estimates_transformed <- data.frame(
estimate = optim_NM$par,
std_error = std_errors
) %>%
mutate(
lower95CI = estimate - 1.96 * std_error,
upper95CI = estimate + 1.96 * std_error
)
estimates_transformed
# Back-transform estimates and confidence intervals
estimates <- exp(estimates_transformed[, -2])
estimates
?distributions
